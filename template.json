{
  "name": "CINDERGRACE ComfyUI + Toolkit",
  "description": "ComfyUI with Cindergrace Toolkit for video generation. Auto-updates on start. Includes Wan 2.2, Flux, LTX-Video support.",
  "dockerImage": "ghcr.io/goettemar/cindergrace-comfyui-runpod:latest",
  "containerDiskInGb": 30,
  "volumeInGb": 0,
  "volumeMountPath": "/workspace",
  "ports": "8188/http,7861/http",
  "env": [
    {
      "key": "SKIP_UPDATE",
      "value": "false"
    },
    {
      "key": "DISABLE_TOOLKIT",
      "value": "false"
    }
  ],
  "startCommand": "/opt/cindergrace/start.sh",
  "readme": "# CINDERGRACE ComfyUI + Toolkit\n\nPre-configured ComfyUI with Cindergrace Toolkit for video generation.\nAuto-updates ComfyUI and custom nodes on every start.\n\n## Services\n\n- **ComfyUI**: `https://<POD_ID>-8188.proxy.runpod.net`\n- **Toolkit**: `https://<POD_ID>-7861.proxy.runpod.net`\n\n## Quick Start\n\n1. **Attach Network Volume** with your models at `/workspace`\n2. **Start the pod**\n3. **Open Toolkit** to download/manage models\n4. **Use ComfyUI** for generation\n\n## Environment Variables\n\n- `SKIP_UPDATE=true` - Skip auto-update (faster start)\n- `DISABLE_TOOLKIT=true` - Don't start toolkit\n\n## Network Volume Structure\n\n```\n/workspace/\n├── models/              # Your models (Network Volume)\n│   ├── clip/\n│   ├── vae/\n│   ├── diffusion_models/\n│   ├── loras/\n│   └── ...\n├── ComfyUI/             # Container-managed\n└── cindergrace_toolkit/ # Auto-cloned on start\n```\n\n## GPU Recommendations\n\n- **RTX 4090/5090** (24-32GB): Wan FP8 + Flux\n- **A100 40GB+**: Full FP16 models\n\n## More Info\n\nhttps://github.com/cindergrace/cindergrace_toolkit\n",
  "category": "AI / ML",
  "minVcpu": 4,
  "minMemoryInGb": 32,
  "minGpuCount": 1,
  "gpuTypes": [
    "NVIDIA RTX 4090",
    "NVIDIA RTX 5090",
    "NVIDIA RTX A5000",
    "NVIDIA RTX A6000",
    "NVIDIA L40",
    "NVIDIA L40S",
    "NVIDIA A100-SXM4-40GB",
    "NVIDIA A100-SXM4-80GB",
    "NVIDIA A100 80GB PCIe"
  ]
}
